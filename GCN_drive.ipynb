{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GCN_drive.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"WHeRx6NchWpr","outputId":"80d8c5ea-bfa9-461a-a759-0a6da91455f5","executionInfo":{"status":"ok","timestamp":1560224909787,"user_tz":420,"elapsed":595,"user":{"displayName":"HAORAN WANG","photoUrl":"","userId":"14589658723335209770"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qLQqxdCohk-i","outputId":"1fa5eef0-0644-4bd7-bbfd-a9309df0978b","executionInfo":{"status":"ok","timestamp":1560224911119,"user_tz":420,"elapsed":573,"user":{"displayName":"HAORAN WANG","photoUrl":"","userId":"14589658723335209770"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["from pathlib import Path\n","ROOT = Path('.')\n","gdrive = ROOT/'gdrive'/'My Drive'/'CS247'/'CS247_Project'\n","\n","list(gdrive.iterdir())"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PosixPath('gdrive/My Drive/CS247/CS247_Project/data'),\n"," PosixPath('gdrive/My Drive/CS247/CS247_Project/src'),\n"," PosixPath('gdrive/My Drive/CS247/CS247_Project/.git'),\n"," PosixPath('gdrive/My Drive/CS247/CS247_Project/.ipynb_checkpoints'),\n"," PosixPath('gdrive/My Drive/CS247/CS247_Project/GCN.ipynb'),\n"," PosixPath('gdrive/My Drive/CS247/CS247_Project/.gitignore'),\n"," PosixPath('gdrive/My Drive/CS247/CS247_Project/.DS_Store'),\n"," PosixPath('gdrive/My Drive/CS247/CS247_Project/data_explore.ipynb'),\n"," PosixPath('gdrive/My Drive/CS247/CS247_Project/GCN_drive.ipynb')]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z8zLxOV2g-Dk","colab":{}},"source":["import math\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","import torch.optim as optim\n","\n","import numpy as np\n","import pandas as pd\n","import argparse\n","import scipy.sparse as sp\n","import pickle\n","import easydict\n","from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n","from sklearn.utils.multiclass import unique_labels\n","from matplotlib import pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aMUtQjl-g-Dp","colab":{}},"source":["class GraphConvolution(Module):\n","    \"\"\"\n","    Simple rGCN layer, similar to https://arxiv.org/abs/1703.06103\n","    :param num_relation: number of different relations in the data\n","    :param num_neighbors: a #relation x #node x 1 matrix that denotes number of neighbors of a node in a relation\n","    :param in_features: number of feature of the input\n","    :param out_features: number of feature of the ouput\n","    :param bias: if bias is added, default is True\n","    :type num_relation: int\n","    :type num_relation: int\n","    :type num_neighbors: array-like object, must be 3 dimension\n","    :type in_features: int\n","    :type out_features: int\n","    :type bias: bool\n","    \"\"\"\n","\n","    def __init__(self, num_relation, num_neighbors, in_features, out_features, bias=True):\n","        super(GraphConvolution, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.adj_weight = Parameter(nn.init.kaiming_uniform_(torch.FloatTensor(num_relation + 1, in_features, out_features)))\n","        self.num_neighbors = num_neighbors\n","#         self.feature_weight = Parameter(nn.init.kaiming_uniform_(torch.FloatTensor(in_features, out_features)))\n","        self.attention = Parameter(nn.init.uniform_(torch.FloatTensor(num_relation + 1)))\n","        if bias:\n","            self.bias = Parameter(torch.FloatTensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.adj_weight.size(2))\n","        # self.weight.data.uniform_(-stdv, stdv)\n","        # self.adj_weight.data.uniform_(-stdv, stdv)\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, input, adjs):\n","        outputs = []\n","        num_node = adjs[0].shape[0]\n","        for i in range(len(self.adj_weight)):\n","            support = torch.mm(input, self.adj_weight[i])\n","            output = torch.spmm(adjs[i], support)\n","#             output = output / self.num_neighbors[i]\n","            output = output * F.softmax(self.attention, dim=0)[i]\n","            outputs.append(output)\n","        output = sum(outputs)\n","#         output = self.normalization(outputs)\n","        # feature_out = torch.mm(input, self.feature_weight)\n","        if self.bias is not None:\n","            return output + self.bias\n","        else:\n","            return output\n","\n","    def normalization(self, embedding: list) -> torch.Tensor:\n","        relations = []\n","        total_sum = 0.0\n","        for emb in embedding:\n","            relations.append(torch.sum(emb, dim=0))\n","        total_sum = sum(relations)\n","        relations = [relation / total_sum for relation in relations]\n","\n","        # print(f\"relation size {len(relations)}\")\n","\n","        relation_normalization = 0\n","        outputs = 0\n","        if embedding[0].device == 'cuda':\n","            relation_normalization = torch.stack(relations, dim=0).cuda()\n","            outputs = torch.stack(embedding, dim=0).cuda()\n","        else:\n","            relation_normalization = torch.stack(relations, dim=0)\n","            outputs = torch.stack(embedding, dim=0)\n","        assert type(relation_normalization) == torch.Tensor\n","        assert type(outputs) == torch.Tensor\n","\n","        attention = []\n","        for i in range(embedding[0].size(1)):\n","            attention.append(self.attention)\n","        attention = torch.stack(attention, dim=1)\n","        attention = F.softmax(attention * relation_normalization, dim=0).reshape(len(embedding), 1, embedding[0].size(1))\n","        outputs = attention * outputs\n","        return torch.sum(outputs, dim=0)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Mh9Ns4AWg-Dr","colab":{}},"source":["class rGCN(nn.Module):\n","    def __init__(self, num_relation, num_neighbors, nfeat, nhid, nclass, dropout):\n","        super(rGCN, self).__init__()\n","\n","        self.gc1 = GraphConvolution(num_relation, num_neighbors, nfeat, nhid)\n","        self.gc2 = GraphConvolution(num_relation, num_neighbors, nhid, nclass)\n","        self.dropout = dropout\n","\n","        \n","    '''\n","    featureless forward function. First x input is a (n x n) all one matrix.\n","    '''\n","    def forward(self, x, adjs):\n","        x_encode = F.relu(self.gc1(x, adjs))\n","        x = F.dropout(x_encode, self.dropout, training=self.training)\n","        x = self.gc2(x, adjs)\n","#         return F.log_softmax(x, dim=1)\n","        return x, x_encode"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FCAEuZszg-Dt","colab":{}},"source":["def accuracy(output, labels):\n","    preds = output.max(1)[1].type_as(labels)\n","    correct = preds.eq(labels).double()\n","    correct = correct.sum()\n","    return correct / len(labels)\n","\n","def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n","    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n","    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n","    indices = torch.from_numpy(\n","        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n","    values = torch.from_numpy(sparse_mx.data)\n","    shape = torch.Size(sparse_mx.shape)\n","    return torch.sparse.FloatTensor(indices, values, shape)\n","\n","def plot_confusion_matrix(y_true, output, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues, filename = None):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    y_pred = output.max(1)[1].type_as(y_true)\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    classes_val = list(classes.keys())\n","    classes_val = unique_labels(classes_val)[unique_labels(y_true, y_pred)]\n","    classes_name = [classes[i] for i in classes_val]\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    #print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes_name, yticklabels=classes_name,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=6.5)\n","    fig.tight_layout()\n","    if filename:\n","        np.save(filename + '.npy', cm)\n","        fig.savefig(filename + '.png')\n","    return ax\n","\n","def top_k_accuracy(output, labels, k):\n","  pred = torch.topk(output, k, dim=1)[1].type_as(labels)\n","  correct = pred.eq(labels.view(-1, 1).expand_as(pred)).double()\n","  correct = correct.sum()\n","  return correct / len(labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MFujNTKbg-Dy","colab":{}},"source":["def train(data, model, optimizer, epoch):\n","#     features, adjs, idx_train, idx_val, labels = data\n","    features, adjs, idx_train, labels, label_mapping = data\n","    for i in range(epoch):\n","        t = time.time()\n","        model.train()\n","        optimizer.zero_grad()\n","        output, encoding = model(features, adjs)\n","#         loss_f = nn.CrossEntropyLoss()\n","#         loss_train = loss_f(output[idx_train], labels[idx_train])\n","        loss_train = hybrid_loss(output[idx_train], encoding[idx_train], labels[idx_train])\n","        acc_train = accuracy(output[idx_train], labels[idx_train])\n","        loss_train.backward()\n","        optimizer.step()\n","  \n","        print('Epoch: {:04d}'.format(i+1),\n","              'loss_train: {:.4f}'.format(loss_train.item()),\n","              'acc_train: {:.4f}'.format(acc_train.item()),\n","              'time: {:.4f}s'.format(time.time() - t))   \n","#         if not args.fastmode:\n","#         if True:\n","#             # Evaluate validation set performance separately,\n","            # deactivates dropout during validation run.\n","#             model.eval()\n","#             output = model(features, adjs)\n","\n","#         loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n","#         acc_val = accuracy(output[idx_val], labels[idx_val])\n","#         print('Epoch: {:04d}'.format(i+1),\n","#               'loss_train: {:.4f}'.format(loss_train.item()),\n","#               'acc_train: {:.4f}'.format(acc_train.item()),\n","#               'loss_val: {:.4f}'.format(loss_val.item()),\n","#               'acc_val: {:.4f}'.format(acc_val.item()),\n","#               'time: {:.4f}s'.format(time.time() - t))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j_s936Wng-D0","colab":{}},"source":["def test(data, model):\n","    features, adj, idx_test, labels, label_mapping = data\n","    model.eval()\n","    output, encoding = model(features, adj)\n","    loss_f = nn.CrossEntropyLoss()\n","#     loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n","#     loss_test = loss_f(output[idx_test], labels[idx_test])\n","    loss_test = hybrid_loss(output[idx_test], encoding[idx_test], labels[idx_test])\n","\n","    acc_test = accuracy(output[idx_test], labels[idx_test])\n","    acc_k_test = top_k_accuracy(output[idx_test], labels[idx_test], 3)\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test.item()),\n","          \"accuracy@3= {:.4f}\".format(acc_k_test.item()))\n","    \n","    classes = label_mapping\n","    labels = labels.clone().cpu()\n","    output = output.clone().cpu()\n","    plot_confusion_matrix(labels[idx_test], output[idx_test], \n","                          classes, normalize=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vw3w2jGMg-D5","colab":{}},"source":["def multi_relation_load(path=\"../data/\", label=\"labels.csv\", \n","                        files=[\"adj_phone.npz\", \"adj_app_installed.npz\", \"adj_app_active.npz\"],\n","                        label_mapping=\"label_mapping\",\n","                        train='train_idx', test='test_idx'):\n","# def multi_relation_load(path=\"../data/\", label=\"full_size_labels.csv\", \n","#                         files=[\"full_size_adj_phone.npz\", \"full_size_adj_app_installed.npz\", \"full_size_adj_app_active.npz\"],\n","#                         label_mapping=\"label_mapping\",\n","#                         train='full_size_train_idx', test='full_size_test_idx'):\n","    print(\"Loading data from path {0}\".format(path))\n","    DATA = Path(path)\n","    ADJS = [DATA/i for i in files]\n","    LABEL = DATA/label\n","    adjs = []\n","    label_df = pd.read_csv(LABEL)\n","    for adj in ADJS:\n","#         adjs.append(sp.load_npz(adj))\n","        adj_tilde = calculate_laplacian(sp.load_npz(adj))\n","        adjs.append(adj_tilde)\n","    \n","    with open(DATA/train, 'rb') as f:\n","        idx_train = pickle.load(f)\n","    with open(DATA/test, 'rb') as f:\n","        idx_test = pickle.load(f)\n","    f.close()\n","    \n","    with open(DATA/label_mapping, 'rb') as f:\n","        label_mapping = pickle.load(f)\n","    \n","    labels = label_df['group'].values\n","    \n","    total_node = adjs[0].shape[0]\n","    edge_indexs = np.array(range(total_node))\n","#     user_node = len(labels)\n","#     user_edge_indexs = np.array(range(user_node))\n","#     self_loop = sp.csr_matrix((np.ones(user_node), (user_edge_indexs, user_edge_indexs)), \n","#                               shape=(total_node, total_node), dtype=np.float32)\n","    self_loop = sp.csr_matrix((np.ones(total_node), (edge_indexs, edge_indexs)), \n","                              shape=(total_node, total_node), dtype=np.float32)\n","    adjs.append(self_loop)\n","\n","    num_neighbors = [adjs[i].sum(1).reshape(total_node, 1) for i in range(len(adjs))]    \n","    # num_neighbors = [np.diff(adjs[i].indptr).reshape(n_entities, 1) for i in range(len(adjs))]\n","    for neighbor in num_neighbors:\n","        neighbor += 1 # smoothing\n","    num_neighbors = [torch.Tensor(neighbors) for neighbors in num_neighbors]\n","\n","    print(\"\\tprocessing features\")\n","    \n","    # edge_indexs = np.array(range(n_entities))\n","    features = sp.csr_matrix((np.ones(total_node), (edge_indexs, edge_indexs)), shape=(total_node, total_node), dtype=np.float32) # dummy feature\n","    \n","    print(\"\\ttransfering into tensors\")\n","\n","    features = sparse_mx_to_torch_sparse_tensor(features)\n","    labels = torch.LongTensor(labels)\n","    adjs = [sparse_mx_to_torch_sparse_tensor(adj) for adj in adjs]\n","\n","    idx_train = torch.LongTensor(idx_train)\n","    idx_test = torch.LongTensor(idx_test)\n","\n","    num_relation = len(files)\n","    return adjs, features, labels, label_mapping, idx_train, idx_test, num_neighbors, num_relation"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9lv04oL9NTVo","colab":{}},"source":["def calculate_laplacian(adj):\n","  total_nodes = adj.shape[0]\n","  adj = adj + sp.identity(total_nodes)\n","  degree = adj.sum(axis=1)\n","  degree_sqrt_inv = np.sqrt(1.0 / degree).reshape(1, -1)\n","  D_sqrt_inv = sp.diags(degree_sqrt_inv, [0], shape=(total_nodes, total_nodes))\n","  adj_tilde = np.dot(np.dot(D_sqrt_inv, adj), D_sqrt_inv)\n","  return adj_tilde"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTqyE6DZVwTR","colab_type":"code","colab":{}},"source":["def calculate_l2(index, output, labels, CUDA=True):\n","    same_label_loss = 0\n","    same_label_count = 0\n","    diff_label_loss = 0\n","    diff_label_count = 0\n","    for i in range(output.shape[0]):\n","        if i == index:\n","            continue\n","        if labels[i] == labels[index]:\n","            same_label_loss += torch.dist(output[i], output[index], 2)\n","            same_label_count += 1\n","        else:\n","            \n","            diff_label_loss += torch.dist(output[i], output[index], 2)\n","            diff_label_count += 1\n","    if same_label_count == 0:\n","        if CUDA:\n","            return torch.Tensor([0]).cuda(0), diff_label_loss / diff_label_count\n","        else:\n","            return torch.Tensor([0]), diff_label_loss / diff_label_count\n","    elif diff_label_count == 0:\n","        if CUDA:\n","            return same_label_loss / same_label_count, torch.zero().cuda(0)\n","        else:\n","            return same_label_loss / same_label_count, torch.Tensor([0])\n","    else:\n","        return same_label_loss / same_label_count, diff_label_loss / diff_label_count\n","        \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyIY44VteHBQ","colab_type":"code","colab":{}},"source":["def hybrid_loss(output, encoding, target):\n","    cnl_f = nn.CrossEntropyLoss()\n","    cnl = cnl_f(output, target)\n","    same_label_loss = 0\n","    diff_label_loss = 0\n","    for i in range(encoding.shape[0]):\n","        sl, dl = calculate_l2(i, encoding, target)\n","        same_label_loss += sl\n","        diff_label_loss += dl\n","  \n","    return cnl + sl - dl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PyTkVM9kg-D8","colab":{}},"source":["import gc\n","def main():\n","#     parser = argparse.ArgumentParser()\n","#     parser.add_argument('--epochs', type=int, default=200,\n","#                         help='Number of epochs to train.')\n","#     parser.add_argument('--lr', type=float, default=0.01,\n","#                         help='Initial learning rate.')\n","#     parser.add_argument('--weight_decay', type=float, default=5e-4,\n","#                         help='Weight decay (L2 loss on parameters).')\n","#     parser.add_argument('--hidden', type=int, default=16,\n","#                         help='Number of hidden units.')\n","#     parser.add_argument('--dropout', type=float, default=0.5,\n","#                         help='Dropout rate (1 - keep probability).')\n","#     parser.add_argument('--data', type=str, default='../../data/twitter_1hop',\n","#                         help='path of the data folder.')\n","#     parser.add_argument('--relations', type=str, default=['friend_list.csv'], action='append',\n","#                         help='edge list files for different relations')\n","\n","#     args = parser.parse_args()\n","\n","    args = easydict.EasyDict({\n","        \"epochs\": 200,\n","        \"lr\": 0.01,\n","        \"weight_decay\": 5e-4,\n","        \"hidden\": 16,\n","        \"dropout\": 0.5,\n","        \"data\": 'gdrive/My Drive/CS247/CS247_Project/data'\n","    })\n","\n","    CUDA = torch.cuda.is_available()\n","    np.random.seed(42)\n","    torch.manual_seed(42)\n","    if CUDA:\n","        torch.cuda.manual_seed(42)\n","\n","    DATA = Path(args.data)\n","#     adjs, features, labels, idx_train, idx_val, idx_test, num_neighbors = multi_relation_load(DATA, files=args.relations)\n","    adjs, features, labels, label_mapping, idx_train, idx_test, num_neighbors, num_relation = multi_relation_load(DATA)\n","    if CUDA:\n","        features = features.cuda(0)\n","        adjs = [i.cuda(0) for i in adjs]\n","        labels = labels.cuda(0)\n","        idx_train = idx_train.cuda(0)\n","#         idx_val = idx_val.cuda(0)\n","        idx_test = idx_test.cuda(0)\n","        num_neighbors = [neighbors.cuda(0) for neighbors in num_neighbors]\n","\n","    rGCN_model = rGCN(num_relation,\n","                num_neighbors,\n","                features.shape[1],\n","                args.hidden,\n","                labels.max().item() + 1,\n","                args.dropout)\n","    optimizer = optim.Adam(rGCN_model.parameters(),\n","                           lr=args.lr, weight_decay=args.weight_decay)\n","\n","\n","    if CUDA:\n","        # rGCN_model = nn.DataParallel(rGCN_model)\n","        rGCN_model.cuda(0)\n","\n","    # t_total = time.time()\n","    print(\"start training\")\n","#     train([features, adjs, idx_train, idx_val, labels], rGCN_model, optimizer, args.epochs)\n","    train([features, adjs, idx_train, labels, label_mapping, ], rGCN_model, optimizer, args.epochs)\n","    # print(\"Optimization Finished!\")\n","    # print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n","    test([features, adjs, idx_test, labels, label_mapping], rGCN_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dckzTakNg-D-","outputId":"09abc014-6709-489b-bc1c-292eb6556633","colab":{"base_uri":"https://localhost:8080/","height":1513}},"source":["main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data from path gdrive/My Drive/CS247/CS247_Project/data\n","\tprocessing features\n","\ttransfering into tensors\n","start training\n","0 0\n","tensor(0.0076, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0072, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0151, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0150, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0203, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0205, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0442, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0492, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0583, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.0957, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0970, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1001, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1073, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1126, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1220, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1243, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1303, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1325, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1369, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1387, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1417, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1434, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1481, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1517, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1539, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1585, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1604, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1640, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1662, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1686, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1712, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1762, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1788, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1815, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1846, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1875, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1889, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1935, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1972, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.1984, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2015, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2078, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2108, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2132, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2157, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2190, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2216, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2261, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2287, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2306, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2332, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2406, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2434, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2504, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2549, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2575, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2597, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2625, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2649, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2684, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2705, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2739, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2757, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2791, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2817, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2853, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2879, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2911, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2929, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2963, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.2968, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3010, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3106, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3148, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3175, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3215, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3220, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3261, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3266, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3310, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3325, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3366, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3424, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3464, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3462, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3510, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3568, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3615, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3649, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3699, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3751, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3798, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3819, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3863, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3867, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3911, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3915, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3959, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.3965, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4007, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4011, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4054, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4142, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4183, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4190, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4232, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4248, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4289, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4295, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4336, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4368, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4412, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4425, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4469, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4479, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4523, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4533, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4573, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4583, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4617, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4661, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4697, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4711, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4746, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4753, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4792, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4812, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4852, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.4963, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5005, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.5008, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5051, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.5058, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5099, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.5109, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5148, device='cuda:0', grad_fn=<AddBackward0>)\n","tensor(0.5157, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5195, device='cuda:0', grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SH7hsvUDnEIP","colab":{}},"source":["x = torch.Tensor([[1, 2, 5], [2, 4, 6], [1, 4, 4]]).cuda(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcUicYVVXqib","colab_type":"code","colab":{}},"source":["y = torch.Tensor([1, 0, 0]).cuda(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"etW-K-48YADN","colab_type":"code","outputId":"cc01389b-72d0-4cdd-d30f-efbb37a30d73","executionInfo":{"status":"ok","timestamp":1560204033182,"user_tz":420,"elapsed":339,"user":{"displayName":"HAORAN WANG","photoUrl":"","userId":"14589658723335209770"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["calculate_l2(0, x, y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.], device='cuda:0'), tensor(2.3428, device='cuda:0'))"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"fUxIZ1-IeA3p","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}